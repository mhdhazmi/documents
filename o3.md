Thanks for the clarification. I’ll review the `/convex` and `/src` directories with a focus on the key app flows: `/src/app/page`, `/src/app/chat/page`, and `/src/app/pdf/[storageId]/pages/page`. I’ll map out the interactions between files and functions in markdown format, highlight any unused or redundant components (e.g., PDF-level OCR), and identify inefficiencies or overhead.

I’ll get started and update you as soon as the analysis is ready.


## **1. Main App Flow Interaction Map**

* **PDF Upload & OCR Initiation (Home Page)** – The upload form in `src/app/page.tsx` handles PDF uploads. When a file is dropped and submitted, the client code:

  * Calls `api.files.mutations.generateUploadUrl` to get a storage URL, then uploads the PDF file.
  * Invokes `api.pdf.mutations.savePdfMetadata` with the file info (storage ID, name, size, page count). This server mutation inserts a record into the `pdfs` table and immediately starts an **OCR workflow** in the background. *(The older approach of directly scheduling PDF-level OCR via `workflowOrchMutation` is now unused and was commented out.)*
  * Finally, the client page navigates to the PDF’s pages view (`/pdf/<id>/pages`) once the metadata is saved.

* **OCR Workflow (Convex Backend)** – The `ocrWorkflow` (defined in `convex/workflow/ocrWorkflow.ts`) orchestrates page-wise OCR:

  * **PDF Splitting:** It first calls `splitPdfIntoPages` to split the uploaded PDF into individual pages. This action fetches the PDF from storage and uses a utility to split it into page blobs, then stores each page and creates a `pages` table entry with a `fileId` (for the page image/PDF). All new page IDs are returned.
  * **Spawn Page OCR Jobs:** For each page ID, `ocrWorkflow` kicks off two **provider workflows** – one for the “gemini” OCR provider and one for “replicate”. These are started in parallel without waiting for completion, allowing concurrent OCR on all pages by both providers.
  * **Post-OCR Concatenation:** After spawning page jobs, the `ocrWorkflow` triggers a **concatenation workflow** (`concatenateWorkflow`) for the PDF (again in the background). This workflow will wait for OCR results to finish (details below) and then handle combining text, embedding, and summarization.

* **Per-Page OCR (Provider Workflows)** – The `providerWorkflow` (in `convex/workflow/providerWorkflow.ts`) encapsulates OCR and text cleanup for **each page** with a specific provider:

  * It runs the provider’s OCR action. For example, for Gemini it calls `ocr/gemini.actions.processPageWithOcr`, which retrieves the page file and sends it to the Gemini API. On success, it writes the extracted text to the `geminiPageOcr` table and marks status “completed” (or “failed” on error). The replicate path is analogous (`replicatePageOcr` table).
  * Next, it runs the OpenAI **cleaning** action for that page’s text. This is implemented via an HTTP endpoint call for streaming. Specifically, `convex/ocr/openai.actions.cleanPage` uses `fetch` to POST to a Convex-defined `/cleanPage` route with the page ID and source (provider). This HTTP route (defined in `convex/api.ts` via `cleanPageHandler`) streams the cleaned/corrected text from OpenAI back to the client and saves the final cleaned text to the `openaiCleanedPage` table: it yields chunks of text as they arrive and, after streaming, stores the full cleaned text with status “completed”.
  * Finally, the provider workflow triggers an **embedding check** by calling `api.ingest.ingest.triggerChunkAndEmbedFromPageCleaning`. This action checks if **all pages** of the PDF are now cleaned for the given provider. If so, it kicks off the chunking & embedding process for the document. (If not, it simply logs that some pages are still pending.) This mechanism ensures that as soon as one of the OCR providers has finished all pages, the document’s text can be embedded for search.

* **Concatenation & Summary Workflow** – The `concatenateWorkflow` (in `convex/workflow/concatenateWorkflow.ts`) runs concurrently to handle combining results and summarization once OCR is done:

  * It periodically checks if all pages have completed OCR+cleaning for at least one provider (Gemini or Replicate). It uses `areAllPagesComplete` queries to test completion status per provider. If not all pages are ready, it schedules itself to retry after a delay (up to a max number of retries to avoid infinite looping).
  * Once all page texts are available (using whichever provider finished first), it **concatenates** the cleaned text of every page in order by querying all `openaiCleanedPage` entries (or falling back to raw OCR text if needed). The concatenated full document text is then saved (via an internal mutation – in practice, this could store to a legacy `openaiOcrResults` or simply be held for summarization).
  * In parallel, the workflow triggers the final steps: it calls `ingest.chunkAndEmbed` to ensure all text chunks are created and embedded in the vector index, and it calls `pdf.mutations.generatePdfSummary` to generate a high-level summary. The chunk-and-embed action will create database **chunks** of text (page-level if available, else document-level) and schedule embedding jobs for them. The summary mutation inserts a `pdfSummaries` entry and schedules an AI model (e.g. GPT) to produce a summary asynchronously. These run in the background; once done, the PDF’s processing can be considered fully complete.

* **PDF Pages View (Client UI)** – The route `src/app/pdf/[storageId]/pages/page.tsx` presents the status and content of the PDF’s pages, reacting to the backend workflow’s progress:

  * It fetches data via Convex queries: `getPdf` for PDF metadata, `getPagesByPdf` for all pages of that PDF (including their OCR status flags and any available text snippet), and `files.getFileDownloadUrl` to retrieve a URL for the original PDF file (used in the viewer). These queries re-run reactively as the database updates (e.g., as pages get marked completed). A context provider (`PdfPageProvider`) wraps the page to keep track of the current page number globally (for sync between the list and viewer).
  * **UI Layout:** The page is split into two sections. On the left, an accordion lists all pages; on the right, a `PDFViewer` displays the PDF content (the actual page images/PDF, loaded from the file URL). A header bar at the top shows the PDF filename, the current page number out of total, and an overall progress bar. The custom `ProgressBarOverall` component computes how many pages are completed by checking the Zustand store for cleaned text chunks (see below).
  * **Page Accordion & Live Text Streaming:** The accordion (`src/components/pageAccordion.tsx`) creates an entry for each page with its number and status:

    * Each page’s status for Gemini and Replicate OCR is displayed with a small colored indicator (`OcrStepperMini`), which reflects “pending/processing/completed/failed” based on the status fields in the `PdfPageInfo` (derived from `geminiPageOcr.ocrStatus` and `replicatePageOcr.ocrStatus`). For example, a green dot indicates a page’s OCR is done.
    * Clicking on a page number (or using navigation) will call `setPage` in context, causing the PDFViewer to jump to that page and ensuring that accordion item opens. The app also visually highlights the current page item.
    * When a page’s accordion item is expanded, the component renders `PageContentWithKicks` for that page. This in turn mounts two `StreamedTextBox` components: one for the Gemini OCR text and one for the Replicate OCR text, each prefaced by a label (e.g. “مغلق المصدر” for the closed-source/Gemini model). On mount, `PageContentWithKicks` invokes the `useKickClean` hook for that page and each provider.
    * **useKickClean Hook:** This hook checks the Convex query for the page’s OCR result status. Once it sees that the OCR status is `"completed"` for the given provider, it initiates streaming of the cleaned text *if it hasn’t already been retrieved*. It does so by calling a client-side helper `streamCleanPage(pageId, provider, onChunk)`, and marks the page as “in flight” to avoid duplicate streams. The `streamCleanPage` function opens a connection to the Convex `/cleanPage` HTTP endpoint (using the public Convex URL) and reads the text stream from the response. As text chunks arrive, it invokes the provided `onChunk` callback with the cumulative text. In our app, the callback updates a Zustand store (`pageStreams`) with the latest text for `pageId_provider` key. This continues until the stream ends, at which point the full cleaned text is stored in the client state and the “in flight” flag is cleared.
    * **StreamedTextBox:** Each `StreamedTextBox` component subscribes to the Zustand store for its page and provider, so it re-renders live as text appears. The user thus sees the page’s OCR text gradually fill in. By design, the **Gemini and Replicate texts are displayed side-by-side** for comparison. If both providers are processing, both boxes will update; if one finishes first, its text shows up while the other might still be blank or “processing”. This interface replaces an older PDF-level OCR view with a more granular, page-level insight.

* **Chat Interaction Flow** – The `src/app/chat/page.tsx` provides a chatbot UI that uses the processed documents for Q\&A (Retrieval-Augmented Generation):

  * **Session Initialization:** On load, a `sessionId` is generated (random UUID) to identify the chat session. This sessionId is used for all queries/mutations so that each chat conversation is isolated. The chat page doesn’t require a selected document upfront – it’s meant to reference any or all uploaded documents as needed.
  * **Data Fetching:** The chat component uses Convex queries to fetch conversation-related data in real time:

    * `serve.retrieveMessages` provides the list of messages (user and assistant) for the current session. This is used by `<ChatMessages>` to render the chat log.
    * `serve.getRagSources` returns an array of source info entries for the session. Each time the assistant answers a question, the backend stores which PDF(s) were cited. The UI uses the **latest entry** (last element in the array) to show sources for the most recent answer. From that entry it extracts `pdfIds` and then queries `pdf.getPdfByIds` to get metadata (like filenames) for those PDFs.
    * If the user clicks on a source or a citation, the state `selectedFileId` and `pdfUrl` are set to load that PDF in the adjacent PDF viewer. The chat UI includes a PDF preview on the left, which uses the same `PDFViewer` component to show the page in question (by calling `PDFViewer.goToPage`). This way, users can read the referenced page alongside the chat answer.
  * **Chat UI Components:**

    * The **Messages list** (`ChatMessages` component) maps over the messages from `retrieveMessages` and renders each via a `ChatMessage` child (which likely formats the bubble and styles). The latest assistant messages may contain citation strings like “(Filename.pdf, p. 3)”. The `onCitationClick` callback is passed down so that clicking on such text calls `handleCitationClick(filename, page)` – this finds the PDF by filename from the fetched `pdfsInfo` and triggers the PDF viewer to show that page.
    * A **Sources** sidebar (`Sources` component) is shown below the messages. It takes the session’s latest `pdfIds` and displays a toggle button for each PDF source. When the user toggles one, it loads that PDF (via setting `selectedPdfId` and fetching its file URL) and lists all page numbers that were referenced from it in the conversation. Each page number is clickable; clicking calls `onPageNavigate(page)` which uses the PDFViewer ref to jump to that page. This offers an alternate way to navigate sources (especially if multiple PDFs were cited in the answer). The Sources component compiles these page references by parsing all assistant messages for citation patterns once new messages arrive – it groups citations by filename and collects unique page numbers.
    * The **Chat input** (`ChatInput` component) at the bottom allows the user to ask questions in Arabic (as hinted by the placeholder). When the user presses Enter or clicks send, it triggers `handleSendMessage`: this calls the Convex mutation `serve.saveMessage` with the text, sessionId, and `isUser:true`. As an immediate response, the code was designed to optimistically append the user’s message to a local state (`setMessages`), but in the current implementation `setMessages` is passed in as a no-op – meaning the UI relies on the reactive query to display the new message once inserted in the DB, rather than local echo. *(This is a minor inefficiency: the local state update is effectively unused.)*
    * There is also a “clear chat” trash icon that simply resets the `sessionId` (generating a new one and thus clearing the messages query) and refreshes the page state.
  * **Backend Q\&A Processing:** The `saveMessage` mutation stores the user message in a `messages` table (with sessionId and timestamp) then immediately schedules an `internal.serve.answer` action to handle it. The heavy lifting happens in `answer` (in `convex/serve/serve.ts`):

    * It fetches all messages for the session (to get conversation context and especially the latest user query). It then inserts a placeholder **bot message** with empty text (and `isUser:false`) via an internal mutation `addBotMessage`, capturing its ID. This placeholder appears in the UI as an empty bubble (ensuring the assistant’s response spot is reserved in sequence).
    * The `answer` action performs a **vector similarity search** to find relevant document text for the query. It uses OpenAI’s embedding API to embed the user’s question into a vector, then queries the `embeddings` vector index for similar vectors (which come from the previously embedded PDF chunks). The search is page-aware: the code can filter for specific PDFs or page ranges if needed, but generally it retrieves the top results across the corpus. The result is a set of chunk IDs with similarity scores.
    * It then looks up the actual chunk documents and their metadata (filename, page number) using `serve.getEnhancedChunks` and computes a list of citation metadata objects. In parallel, it records the PDFs involved by calling `updateRagSources` with the sessionId and the list of PDF IDs that came up. This inserts a new entry in the `ragSources` table linking the current question to those sources. (On the client, `getRagSources` will refresh and the Sources component will pick up the new PDFs.)
    * Next, the action constructs a **prompt for OpenAI**. It includes a system role instruction (in Arabic, guiding the assistant to use citations) and attaches each retrieved text chunk as a system message with a citation tag prepended (e.g. “Content from (Report.pdf, p. 2): \[excerpt]”). Then it appends the conversation history (user and assistant messages) in the appropriate format. By providing the documents content with citations already in place, the model is encouraged to produce an answer that cites those sources.
    * The assistant’s answer is generated via `streamText` using the GPT-4 model (denoted `"gpt-4o"` in config). Importantly, this uses OpenAI’s streaming API so that we can receive partial results. The code iterates over the `result.textStream` asynchronously, and for each new chunk of text, it **updates the placeholder bot message** in the database via `updateBotMessage`. The UI (ChatMessages component) is subscribed to the messages query, so it sees the bot message text field being updated in real-time, causing the partial answer to appear and incrementally grow in the chat bubble. This yields a streaming chat experience for the user.
    * Once the stream is done, the full answer is stored in the messages table (as the final state of that bot message). If an error occurs, the code catches it and updates the bot message text to an error apology.
    * The end result is a complete Q\&A exchange: the user’s question and the assistant’s answer (with citations). The cited documents are logged in `ragSources` (for source list UI) and the conversation can continue with follow-up questions (since all messages are retained and sent to the model for context on each turn).

## **2. Unused or Legacy Components (PDF-level OCR)**

* **Legacy PDF-Level OCR Pipeline:** The repository still contains code for an older approach that performed OCR on the entire PDF as one unit, which has been superseded by the page-by-page workflow. For example, `convex/ocr/gemini/actions.ts` defines `processPdfWithOcr` which uploads the whole PDF to Google’s OCR API (Gemini) and stores results in a single `geminiOcrResults` entry. Similarly, `replicate/actions.ts` has a `processPdfWithOcr`. In the current app flow, these are **never called** – instead, the `savePdfMetadata` mutation launches the page-wise `ocrWorkflow` immediately, and that uses `processPageWithOcr` for each page. The old `workflowOrch` (in `convex/workflowOrch.ts`) which concurrently kicked off `processPdfWithOcr` for Gemini and Replicate is effectively deprecated (the front-end call to it is commented out). These PDF-level OCR functions and the `geminiOcrResults`/`replicateOcrResults` tables can be considered legacy. They were likely kept for backup or transitional purposes but are safe to remove if all documents are processed with the new method.

* **Whole-Document Cleaning and Streaming:** Alongside PDF-level OCR, there is legacy support for cleaning the entire OCR text. The Convex HTTP endpoint `/clean` (handled by `cleanHandler`) and the `openaiOcrResults` table were used to clean and store full-document text. Now, cleaning is done per page via `/cleanPage` and stored in `openaiCleanedPage`. The new page workflow doesn’t populate `openaiOcrResults` at all (except possibly via the concatenate workflow for backward compatibility). Unless the team needs to support already-processed PDFs from the old method, the PDF-level cleaning endpoint and related code are no longer needed.

* **UI Components for PDF-level Status:** The UI originally had components to indicate OCR progress at a document level (e.g., showing “Uploaded -> Processing -> Completed” steps). The file `src/app/pdf/[storageId]/components/OcrProgressStepper.tsx` defines an elaborate step indicator UI, and `MinimalistProgressBar.tsx` was a compact variant. In the current interface, **neither is utilized** – the pages view uses its own progress bar (`ProgressBarOverall`) and per-page indicators. A search through the codebase shows no references to where `OcrProgressStepper` is rendered. These components appear to be vestiges of a previous design iteration and can be removed or ignored. The active code has shifted focus to page-level status (via `OcrStepperMini` in the accordion).

* **Redundant State in Chat Input:** The `ChatInput` component was built with local state handling in mind (it accepts a `setMessages` prop to immediately append the user’s question to a local array). However, in the current usage, the parent passes `setMessages={() => {}}` – essentially a dummy function. This means the code inside `ChatInput` that tries to optimistically update messages does nothing. The chat relies entirely on the Convex query to update the message list. This doesn’t break functionality, but it is **dead code** from an earlier approach (perhaps initially intended to make the UI feel snappier). It can be cleaned up to avoid confusion – either implement local echo properly or remove the unused state/update logic.

* **PDF Status Field:** Each PDF in the `pdfs` table has a `status` field (e.g., “uploaded”, “processing”, “completed”). In the new flow, this field isn’t actively managed. `savePdfMetadata` sets `status: "uploaded"` on creation. The old PDF-level OCR would have updated this to “processing” and “completed”, but the page-level workflow doesn’t currently update `pdfs.status` at all (it focuses on page statuses). As a result, this field likely remains “uploaded” even after processing. It’s effectively unused in UI (the pages view uses page statuses instead). Unless needed for an overall status label, this is legacy metadata. We could update it at the end of the workflow or drop it entirely in favor of deriving status from pages.

* **Miscellaneous Unused Code Paths:** Other minor bits include a `pdf.getPdfList` query (to filter PDFs by status or name) which may not be used in any UI, and some config flags or feature toggles in the code (e.g., a mention of “Skip if feature flag is not enabled” in `splitPdfIntoPages`, though no actual flag check is implemented). These suggest the codebase had experimental or toggled features that aren’t active now. They don’t affect the main flows and could be cleaned up to streamline the code.

**Conclusion:** The critical legacy elements to identify are the **monolithic PDF OCR and cleaning logic** and their associated UI components. The current architecture operates on a finer granularity (pages) and improved real-time feedback, making the old components safe to remove. Before removal, one should ensure no edge-case or admin feature is secretly using them, but based on our review, they are not wired into the user-facing flows. Pruning these will reduce complexity and avoid confusion, focusing the project on the page-level OCR paradigm that’s in use.

## **3. Inefficiencies, Duplication, and Architectural Overhead**

* **Dual OCR Pipelines (Legacy vs New):** As noted, the coexistence of old PDF-level OCR code with the new page-level system is an overhead. It introduces duplicate pathways for accomplishing OCR. For example, there are **two sets of tables** (`geminiOcrResults` vs `geminiPageOcr`, etc.) and even two streaming endpoints (`/clean` vs `/cleanPage`). Maintaining both increases complexity and potential for inconsistency. Removing or disabling the old pipeline entirely will eliminate this duplication. In practice, the new workflow already covers the functionality more efficiently, so the legacy pipeline can be safely dropped to reduce code bloat.

* **Workflow Complexity & Redundancy:** The introduction of Convex “workflow” constructs has improved structure (e.g., `ocrWorkflow`, `providerWorkflow`, etc.), but there is some redundancy in how tasks are triggered:

  * The **provider workflows** each call `triggerChunkAndEmbedFromPageCleaning` after finishing a page, and the **concatenate workflow** also calls `chunkAndEmbed` when the whole PDF is ready. This means the embedding process might be invoked twice (one potentially no-op). In practice, `chunkAndEmbed` is idempotent and checks if chunks already exist or if pages are complete, but the control flow could be streamlined. It’s an overhead to have two separate mechanisms racing to kick off embedding. Choosing one strategy (e.g., rely solely on the concatenate workflow to call `chunkAndEmbed` when either source is done, and remove the per-page trigger) would simplify orchestration without losing functionality.
  * The **summary generation** is currently parallel to embedding, which is good, but consider that the summary uses the concatenated text. The concatenation is done in the workflow and summary is started immediately after. If the summary were instead generated after embedding, it might allow using embeddings or other info – but this is not a significant issue, just an architectural note. The main inefficiency was the duplicate triggers for embedding, which can be refactored.

* **Database Query Patterns:** In `getPagesByPdf`, for each page the code individually queries the OCR status tables. For a PDF with many pages, this results in N separate queries (one per page for gemini, one per page for replicate, and also one per page for cleaned text). This is somewhat inefficient. A more optimal approach would be to query each status table by `pdfId` once (using the indexes we have) and map the results in memory. For example, fetch all `geminiPageOcr` where `pdfId == X` and index by pageId. The current approach was likely simpler to implement but could lead to extra DB round-trips. If performance becomes a concern with large documents, refactoring this query to reduce looped calls would be beneficial. That said, Convex might handle some batching under the hood, and for PDFs of a few dozen pages this overhead is minor – it’s more about clarity and best practice.

* **State Management & Streaming:** The use of a client-side store (`pageStreams` via Zustand) for streaming OCR results is clever, but it introduces a slight complexity: we’re maintaining UI state that duplicates data in the database (cleaned text). For instance, once streaming is done, the full cleaned text resides both in Convex (openaiCleanedPage) and in the client store. The app doesn’t currently write the full text back into a more permanent UI state; it just keeps it in memory. If the user refreshes the page after OCR is complete, the `getPagesByPdf` query would provide the `cleanedSnippet` (160 chars) but not the entire text, and the UI might not show all text without re-triggering `streamCleanPage`. In effect, the system favors real-time streaming over storing full page text on the client. This is a design choice, but it means the **snippet** is the only text from Convex delivered to the client on load. If a user wants to see the whole text after the fact, the app currently would re-run the stream or fetch it via the HTTP endpoint again. This could be seen as an inefficiency or missing feature (perhaps the snippet was used to avoid sending very large texts to the client initially). A possible improvement is: once cleaning is done, populate a field with the cleaned text or increase the snippet length, so the UI can instantly show the static final text on page load, and only stream if the cleaning is still in progress. This would save re-contacting the endpoint for already completed pages. As it stands, the snippet logic serves as a placeholder – it’s useful for quick preview and search indexing, but not for reading. Depending on requirements, you might remove the snippet and have the UI always stream (simpler, but more load if users navigate away/back), or store full text in state (more initial payload). This is a trade-off to consider.

* **Data Duplication in Chat Sources:** The `ragSources` implementation stores a new entry for each answer, accumulating an array over the session. Over a long chat, this array grows, and each `getRagSources` query sends the entire history of source lists. In the UI, however, only the *last* entry is used. This means we’re repeatedly sending older entries that aren’t needed client-side. It’s a minor inefficiency in terms of bandwidth and processing. A simpler approach might be to just store/update a single record per session with the latest `pdfIds` used (or store sources as part of the message record). Alternatively, the client could request only the last item. Right now, after every question, the Sources component does `sourcesData[sourcesData.length - 1]`, ignoring the rest. This could be refactored to avoid pulling unused data. Not critical for functionality, but something to note for optimization.

* **Error Handling and Race Conditions:** The system largely assumes both OCR providers will run for each page. There is some potential overhead in doing dual OCR (Gemini *and* Replicate) if ultimately the app might only use one source’s text. The code chooses the fastest completed provider’s text for embedding and summary, which is efficient, but it means the slower provider’s work might end up unused (except for comparison in the UI by the user). Running two OCRs is intentional for quality comparison, but it doubles the workload. If computational resources or API costs are a concern, one might allow selecting only one provider. This is more of a product decision than code inefficiency, but worth mentioning. On the code side, both provider workflows run uncoordinated – which is fine – but we should ensure that if one fails, the other can still drive completion. The `concatenateWorkflow` will pick up whichever finished. This design is sound (no single point of failure), at the cost of doing extra work. It’s an overhead that the team likely accepts for better OCR results.

* **Cleanup of Temporary States:** After a PDF is fully processed, there might be some cleanup tasks that are not yet automated. For example, `messages` and `ragSources` for chat sessions are not cleared except when a user manually hits the clear button – over many sessions, the DB could accumulate a lot of chat history (which might be fine, or might need a retention policy). Similarly, if the system reprocesses a PDF (if that were possible), it might insert duplicate pages or OCR entries. Currently, `savePdfMetadata` does not check for duplicates; it always inserts a new PDF record and triggers processing. This could lead to inefficiency if users upload the same file multiple times. Addressing such edge cases (e.g., detecting duplicate file hashes to avoid re-processing) could improve efficiency but would add complexity. Mentioning this as an architectural consideration: as the app grows, consider strategies for deduping or cleaning up old data to keep queries snappy.

* **Front-End Responsiveness:** The chat UI currently waits for the Convex query to update to display the user’s message. As identified, the local message state update is disabled, which can cause a slight delay (usually very small) in showing the user’s own message. Re-enabling an immediate echo (and perhaps a loading indicator for the bot reply) could improve perceived performance. This is a minor UI inefficiency. It doesn’t affect system throughput but does affect user experience flow.

**Recommendations:** Focus on **removing deprecated code** to reduce maintenance overhead (the PDF-level OCR and its UI feedback). Simplify the workflows by eliminating duplicate triggers and consolidating status updates. Optimize Convex queries that scale with number of pages if large documents are expected (batch where possible). And consider adjustments in state handling: once a page’s text is fully cleaned, you might cache or store that text for instant retrieval, rather than always requiring a streaming fetch on component mount. These changes will make the system leaner and more straightforward to reason about, without altering the core functionality.
